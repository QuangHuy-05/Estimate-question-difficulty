{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vncorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327dd942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "import json, re\n",
    "from tqdm import tqdm \n",
    "import nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3aa680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file = r\"D:\\NCKH\\estimate_question_difficulty\\data_processed\\vƒÉn_processed.txt\"\n",
    "# output_file = r\"D:\\NCKH\\estimate_question_difficulty\\Embedding\\Output_ws\\qa_processed_ws.txt\"\n",
    "\n",
    "# def word_segment(input_file, output_file):\n",
    "#     rdrsegmenter = VnCoreNLP(\n",
    "#     r\"D:\\Download\\VnCoreNLP-1.1.1\\VnCoreNLP-1.1.1\\VnCoreNLP-1.1.1.jar\",\n",
    "#     annotators=\"wseg\",\n",
    "#     max_heap_size='-Xmx2g'\n",
    "#     )   \n",
    "#     with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#         lines = f.readlines()\n",
    "#     with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         for line in tqdm(lines, desc=\"Word Segmenting\"):\n",
    "#             line = line.strip()\n",
    "#             if line:\n",
    "#                 segmented = rdrsegmenter.tokenize(line)\n",
    "#                 f.write(\" \".join(segmented[0]) + \"\\n\")\n",
    "#     print(\"‚úÖ Word segmentation done.\")\n",
    "def word_segment(input_file, output_file):\n",
    "    rdrsegmenter = VnCoreNLP(\n",
    "        r\"D:\\Download\\VnCoreNLP-1.1.1\\VnCoreNLP-1.1.1\\VnCoreNLP-1.1.1.jar\",\n",
    "        annotators=\"wseg\",\n",
    "        max_heap_size='-Xmx2g'\n",
    "    )   \n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(lines, desc=\"Word Segmenting\"):\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                segmented = rdrsegmenter.tokenize(line)\n",
    "                seg_line = \" \".join(segmented[0])\n",
    "                # üîπ cleanup: x√≥a kho·∫£ng tr·∫Øng quanh \"_\"\n",
    "                seg_line = re.sub(r\"\\s*_\\s*\", \"_\", seg_line)\n",
    "                f.write(seg_line + \"\\n\")\n",
    "    print(\"‚úÖ Word segmentation done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8522cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import json, re\n",
    "\n",
    "def ws_question(input_file, output_file_ws):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    blocks = re.split(r\"(?=C√¢u\\s*\\d+\\s*:)\", text)  # t√°ch theo \"C√¢u s·ªë:\"\n",
    "    questions = []\n",
    "\n",
    "    for i, block in enumerate(blocks, 1):\n",
    "        block = block.strip()\n",
    "        if not block: \n",
    "            continue\n",
    "\n",
    "        # id\n",
    "        id_match = re.search(r\"C√¢u\\s*(\\d+)\\s*:\", block)\n",
    "        qid = int(id_match.group(1)) if id_match else i\n",
    "\n",
    "        # subject\n",
    "        subject = re.search(r\"M√¥n\\s*:\\s*(.+)\", block)\n",
    "        subject = subject.group(1).strip() if subject else None\n",
    "\n",
    "        # question\n",
    "        question = re.search(r\"C√¢u h·ªèi\\s*:\\s*(.+?)(?=\\n\\s*[A-D]\\.|ƒê√°p|Gi·∫£i th√≠ch|$)\", block, re.S)\n",
    "        question = question.group(1).strip().replace(\"\\n\", \" \") if question else None\n",
    "\n",
    "        options = re.findall(r\"([A-D])\\.\\s*(.+)\", block)\n",
    "        # lo·∫°i b·ªè tr√πng l·∫∑p gi·ªØ nguy√™n th·ª© t·ª±\n",
    "        seen, options_list = set(), []\n",
    "        for k, v in options:\n",
    "            opt = f\"{k}. {v.strip()}\"\n",
    "            if opt not in seen:\n",
    "                options_list.append(opt)\n",
    "                seen.add(opt)\n",
    "\n",
    "        ans = re.search(r\"ƒê√°p[_ ]?√°n ƒë√∫ng\\s*:\\s*([A-D])\\.\\s*(.+)\", block)\n",
    "        answer = f\"{ans.group(1)}. {ans.group(2).strip()}\" if ans else None\n",
    "\n",
    "\n",
    "        if subject and question and options_list and answer:\n",
    "            questions.append({\n",
    "                \"id\": qid,\n",
    "                \"subject\": subject,\n",
    "                \"question\": question,\n",
    "                \"options\": options_list,\n",
    "                \"answer\": answer\n",
    "            })\n",
    "\n",
    "    with open(output_file_ws, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(questions, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ chuy·ªÉn {len(questions)} c√¢u h·ªèi sang JSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf74ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def ori_question(input_file, output_file_ori):\n",
    "    # ƒê·ªçc file txt\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # T√°ch c√°c block c√¢u h·ªèi b·∫±ng chu·ªói g·∫°ch ngang d√†i\n",
    "    blocks = [b.strip() for b in text.split(\"--------------------------------------------------\") if b.strip()]\n",
    "    questions = []\n",
    "    for i, block in enumerate(blocks, start=1):\n",
    "        block = block.strip()\n",
    "        if not block: \n",
    "            continue\n",
    "        # id\n",
    "        id_match = re.search(r\"C√¢u\\s*(\\d+)\\s*:\", block)\n",
    "        qid = int(id_match.group(1)) if id_match else i\n",
    "        \n",
    "        subject_match = re.search(r\"M√¥n\\s*:\\s*(.+)\", block)\n",
    "        subject = subject_match.group(1).strip() if subject_match else None\n",
    "\n",
    "        question_match = re.search(r\"C√¢u h·ªèi\\s*:\\s*(.+)\", block)\n",
    "        question = question_match.group(1).strip() if question_match else None\n",
    "\n",
    "        options = re.findall(r\"([A-D])\\.\\s*(.+)\", block)\n",
    "        # lo·∫°i b·ªè tr√πng l·∫∑p gi·ªØ nguy√™n th·ª© t·ª±\n",
    "        seen, options_list = set(), []\n",
    "        for k, v in options:\n",
    "            opt = f\"{k}. {v.strip()}\"\n",
    "            if opt not in seen:\n",
    "                options_list.append(opt)\n",
    "                seen.add(opt)\n",
    "\n",
    "        ans = re.search(r\"ƒê√°p[_ ]?√°n ƒë√∫ng\\s*:\\s*([A-D])\\.\\s*(.+)\", block)\n",
    "        answer = f\"{ans.group(1)}. {ans.group(2).strip()}\" if ans else None\n",
    "\n",
    "        if subject and question and options_list and answer:\n",
    "            questions.append({\n",
    "                \"id\": qid,\n",
    "                \"subject\": subject,\n",
    "                \"question\": question,\n",
    "                \"options\": options_list,\n",
    "                \"answer\": answer,\n",
    "            })\n",
    "\n",
    "    with open(output_file_ori, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(questions, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ chuy·ªÉn th√†nh c√¥ng {len(questions)} c√¢u h·ªèi sang file json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
