{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55115092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vncorenlp nbimporter\n",
    "# !pip install pandas transformers torch underthesea vncorenlp tqdm hf_xet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028fdfb8",
   "metadata": {},
   "source": [
    "data_process->word_segment ->Ã£Ã£Ã£Ã£(emb_different + complex) -> merged -> training-> check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43e92a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "import importlib\n",
    "\n",
    "from vncorenlp import VnCoreNLP\n",
    "import json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#import m√¥ h√¨nh\n",
    "from underthesea import word_tokenize, pos_tag\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "#import file\n",
    "from data_process import run_data_processed, clean_question, clean_word\n",
    "from Embedding.word_segment import word_segment, ws_question, ori_question\n",
    "from Embedding.emb_different import get_diff\n",
    "from Embedding.sen_complex import run_sen_comlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17919d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process():\n",
    "    subject_files = {\n",
    "    \"VƒÉn_h·ªçc\": \"output/vƒÉn.json\",\n",
    "    \"s·ª≠\": \"output/s·ª≠.json\",\n",
    "    \"ƒë·ªãa\": \"output/ƒë·ªãa.json\",\n",
    "    \"anh\": \"output/anh_vƒÉn.json\"\n",
    "    }\n",
    "    all_data = {}\n",
    "    for subject, file_path in subject_files.items():\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                all_data[subject] = json.load(f)\n",
    "            print(f\"Loaded {len(all_data[subject])} questions from {subject}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found - {file_path}\")\n",
    "            continue\n",
    "    qa_pairs = []\n",
    "    processed_qa = []\n",
    "    clean_question(qa_pairs, all_data)\n",
    "    clean_word(processed_qa, qa_pairs)\n",
    "    run_data_processed(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2274796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_embedding():\n",
    "    input_file = r\"data_processed\\VƒÉn_h·ªçc_processed.txt\"\n",
    "    output_file = r\"Embedding\\Output_ws\\qa_processed_ws.txt\"\n",
    "    print(\"---------ƒêang x·ª≠ l√Ω Word Segment--------------\\n\")\n",
    "    word_segment(input_file, output_file)\n",
    "    print(\"---------ƒê√£ x·ª≠ l√Ω xong Word Segment----------\\n\")\n",
    "\n",
    "    #------------- x·ª≠ l√Ω data word segment--------------------------------\n",
    "    print(\"---------ƒêang x·ª≠ l√Ω ws_question--------------\\n\")\n",
    "    output_file_ws = r\"Embedding\\Output_ws\\questions.json\"\n",
    "    ws_question(output_file, output_file_ws)\n",
    "    print(\"---------ƒê√£ x·ª≠ l√Ω xong ws_question----------\\n\")\n",
    "\n",
    "    #-------------clean data nguy√™n b·∫£n-----------------------\n",
    "    print(\"---------ƒêang x·ª≠ l√Ω ori_question--------------\\n\")\n",
    "    output_file_ori = r\"Embedding\\Output_ws\\ori_questions.json\"\n",
    "    ori_question(input_file, output_file_ori)\n",
    "    print(\"---------ƒê√£ x·ª≠ l√Ω xong ori_question----------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa3278e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_different_feartures():\n",
    "    input = r\"Embedding\\Output_ws\\questions.json\"\n",
    "    output = r\"Embedding\\Output_features\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "    model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "    print(f\"---------ƒêang t√≠nh to√°n ƒë·ªô l·ªách embedding gi·ªØa c√°c options-------------\\n\")\n",
    "    get_diff(input, output, tokenizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d972142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lit_terms(txt_file=\"lit_terms.txt\"):\n",
    "    \"\"\"\n",
    "    ƒê·ªçc danh s√°ch thu·∫≠t ng·ªØ t·ª´ file txt th√†nh set\n",
    "    \"\"\"\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        terms = {line.strip().lower() for line in f if line.strip()}\n",
    "    print(f\"üìñ ƒê√£ load {len(terms)} thu·∫≠t ng·ªØ t·ª´ {txt_file}\")\n",
    "    return terms\n",
    "\n",
    "def create_sentence_comlex_feartures():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"‚ö° Using device:\", device)\n",
    "    # PhoBERT cho embedding\n",
    "    phobert_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "    phobert_model = AutoModel.from_pretrained(\"vinai/phobert-base\").to(device)\n",
    "    phobert_model.eval()\n",
    "    # GPT2 ti·∫øng Vi·ªát cho perplexity\n",
    "    gpt2_tokenizer = AutoTokenizer.from_pretrained(\"NlpHUST/gpt2-vietnamese\")\n",
    "    gpt2_model = AutoModelForCausalLM.from_pretrained(\"NlpHUST/gpt2-vietnamese\").to(device)\n",
    "    gpt2_model.eval()\n",
    "\n",
    "    connectors = {\"v√†\",\"ho·∫∑c\",\"nh∆∞ng\",\"tuy nhi√™n\",\"c≈©ng nh∆∞ l√†\",\"n√™n\",\"m√†\",\n",
    "              \"n·∫øu\",\"th√¨\",\"b·ªüi v√¨\",\"v√¨\",\"khi\",\"m·∫∑c d√π\",\"ƒë·ªÉ\",\"sau\",\"sau_khi\",\n",
    "              \"tr∆∞·ªõc\",\"tr∆∞·ªõc khi\",\"hay\",\"do\",\"do ƒë√≥\",\"h·ªÖ\",\"l·∫´n\",\"c√πng\",\n",
    "              \"ngo√†i ra\",\"v·∫≠y\",\"tr·ª´ phi\",\"h∆°n\",\"b·∫±ng\",\"nh∆∞\",\"d√π cho\",\n",
    "              \"nh·∫±m\",\"v√¨ v·∫≠y\",\"ng∆∞·ª£c l·∫°i\",\"ch·ªâ\",\"ch·ªâ tr·ª´\"}\n",
    "    lit_terms = load_lit_terms(\"lit_terms.txt\")\n",
    "    svd = TruncatedSVD(n_components=128) \n",
    "    input_file = r\"Embedding\\Output_ws\\ori_questions.json\"\n",
    "    output_file = r\"Embedding\\Output_features\\question_features.csv\"\n",
    "    print(\"------------------------ƒêang t√≠nh to√°n ƒë·ªô l·ªách gi·ªØa c√°c options------------------------\")\n",
    "    run_sen_comlex(input_file,output_file, connectors, phobert_tokenizer\n",
    "                   , phobert_model, gpt2_tokenizer, gpt2_model, device, lit_terms, svd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec6b21b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merger_data():\n",
    "    print(f\"--------------------------------------ƒêang k·∫øt h·ª£p data--------------------------------------\")\n",
    "    df1 = pd.read_csv(r\"Embedding\\Output_features\\noise_features.csv\")\n",
    "    df2 = pd.read_csv(r\"Embedding\\Output_features\\question_features.csv\")\n",
    "    df3 = pd.read_csv(r\"training\\data_for_training\\van_with_bloom_out.csv\")\n",
    "    merged = pd.merge(df1, df2, on = \"id\", how = \"inner\")\n",
    "    merged = pd.merge(merged, df3, on = \"id\", how = \"inner\")\n",
    "    result = merged.drop(columns=[\"question_x\", \"answer\", \"subject\", \"question_y\",\"question\"])\n",
    "    result.to_csv(\"merged.csv\", index = False)\n",
    "    print(\"Done.........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1ef2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lit_terms():\n",
    "    df1 = pd.read_csv(r\"bold_words.csv\", encoding=\"utf-8-sig\")\n",
    "    df2 = pd.read_csv(r\"bold_words_2.csv\", encoding = \"utf-8-sig\")\n",
    "\n",
    "    df1[\"Bold Words\"] = df1[\"Bold Words\"].str.lower()\n",
    "    df2[\"Bold Words\"] = df2[\"Bold Words\"].str.lower()\n",
    "\n",
    "    merged = pd.concat([df1, df2], ignore_index=True)\n",
    "    unique_words = merged.drop_duplicates(subset=[\"Bold Words\"], keep=\"first\")[\"Bold Words\"]\n",
    "    # Xu·∫•t TXT\n",
    "    output_file = \"lit_terms.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for word in unique_words:\n",
    "            f.write(word.strip() + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ t·∫°o file {output_file}\")\n",
    "    print(\"üî¢ S·ªë l∆∞·ª£ng t·ª´ duy nh·∫•t:\", len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc582fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------ƒêang k·∫øt h·ª£p data--------------------------------------\n",
      "Done.........\n"
     ]
    }
   ],
   "source": [
    "data_process()\n",
    "run_embedding()\n",
    "\n",
    "create_lit_terms()\n",
    "create_different_feartures()\n",
    "create_sentence_comlex_feartures()\n",
    "merger_data()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
