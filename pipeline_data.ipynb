{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55115092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vncorenlp nbimporter\n",
    "# !pip install pandas transformers torch underthesea vncorenlp tqdm hf_xet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028fdfb8",
   "metadata": {},
   "source": [
    "data_process->word_segment ->Ã£Ã£Ã£Ã£(emb_different + complex) -> merged -> training-> check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43e92a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import importlib\n",
    "\n",
    "from vncorenlp import VnCoreNLP\n",
    "import json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#import m√¥ h√¨nh\n",
    "from underthesea import word_tokenize, pos_tag\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "#import file\n",
    "from data_process import run_data_processed, clean_question, clean_word\n",
    "from Embedding.word_segment import word_segment, ws_question, ori_question\n",
    "from Embedding.emb_different import get_diff\n",
    "from Embedding.sen_complex import run_sen_comlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17919d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process():\n",
    "    subject_files = {\n",
    "    \"VƒÉn_h·ªçc\": \"output/vƒÉn.json\",\n",
    "    # \"s·ª≠\": \"output/s·ª≠.json\",\n",
    "    # \"ƒë·ªãa\": \"output/ƒë·ªãa.json\",\n",
    "    # \"anh\": \"output/anh_vƒÉn.json\"\n",
    "    }\n",
    "    all_data = {}\n",
    "    for subject, file_path in subject_files.items():\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                all_data[subject] = json.load(f)\n",
    "            print(f\"Loaded {len(all_data[subject])} questions from {subject}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found - {file_path}\")\n",
    "            continue\n",
    "    qa_pairs = []\n",
    "    processed_qa = []\n",
    "    clean_question(qa_pairs, all_data)\n",
    "    clean_word(processed_qa, qa_pairs)\n",
    "    run_data_processed(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4337f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_embedding():\n",
    "    input_file = r\"data_processed\\VƒÉn_h·ªçc_processed.txt\"\n",
    "    output_file = r\"Embedding\\Output_ws\\qa_processed_ws.txt\"\n",
    "    print(\"---------ƒêang x·ª≠ l√Ω Word Segment--------------\\n\")\n",
    "    word_segment(input_file, output_file)\n",
    "    print(\"---------ƒê√£ x·ª≠ l√Ω xong Word Segment----------\\n\")\n",
    "\n",
    "    #------------- x·ª≠ l√Ω data word segment--------------------------------\n",
    "    print(\"---------ƒêang x·ª≠ l√Ω ws_question--------------\\n\")\n",
    "    output_file_ws = r\"Embedding\\Output_ws\\questions.json\"\n",
    "    ws_question(output_file, output_file_ws)\n",
    "    print(\"---------ƒê√£ x·ª≠ l√Ω xong ws_question----------\\n\")\n",
    "\n",
    "    #-------------clean data nguy√™n b·∫£n-----------------------\n",
    "    print(\"---------ƒêang x·ª≠ l√Ω ori_question--------------\\n\")\n",
    "    output_file_ori = r\"Embedding\\Output_ws\\ori_questions.json\"\n",
    "    ori_question(input_file, output_file_ori)\n",
    "    print(\"---------ƒê√£ x·ª≠ l√Ω xong ori_question----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa3278e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_different_feartures():\n",
    "    input = r\"Embedding\\Output_ws\\questions.json\"\n",
    "    output = r\"Embedding\\Output_features\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "    model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "    print(f\"---------ƒêang t√≠nh to√°n ƒë·ªô l·ªách embedding gi·ªØa c√°c options-------------\\n\")\n",
    "    get_diff(input, output, tokenizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d972142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lit_terms(txt_file=r\"create_lit_terms\\lit_terms.txt\"):\n",
    "    \"\"\"\n",
    "    ƒê·ªçc danh s√°ch thu·∫≠t ng·ªØ t·ª´ file txt th√†nh set\n",
    "    \"\"\"\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        terms = {line.strip().lower() for line in f if line.strip()}\n",
    "    print(f\"üìñ ƒê√£ load {len(terms)} thu·∫≠t ng·ªØ t·ª´ {txt_file}\")\n",
    "    return terms\n",
    "\n",
    "def create_sentence_comlex_feartures():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"‚ö° Using device:\", device)\n",
    "    # PhoBERT cho embedding\n",
    "    phobert_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "    phobert_model = AutoModel.from_pretrained(\"vinai/phobert-base\").to(device)\n",
    "    phobert_model.eval()\n",
    "    # GPT2 ti·∫øng Vi·ªát cho perplexity\n",
    "    gpt2_tokenizer = AutoTokenizer.from_pretrained(\"NlpHUST/gpt2-vietnamese\")\n",
    "    gpt2_model = AutoModelForCausalLM.from_pretrained(\"NlpHUST/gpt2-vietnamese\").to(device)\n",
    "    gpt2_model.eval()\n",
    "\n",
    "    connectors = {\"v√†\",\"ho·∫∑c\",\"nh∆∞ng\",\"tuy nhi√™n\",\"c≈©ng nh∆∞ l√†\",\"n√™n\",\"m√†\",\n",
    "              \"n·∫øu\",\"th√¨\",\"b·ªüi v√¨\",\"v√¨\",\"khi\",\"m·∫∑c d√π\",\"ƒë·ªÉ\",\"sau\",\"sau_khi\",\n",
    "              \"tr∆∞·ªõc\",\"tr∆∞·ªõc khi\",\"hay\",\"do\",\"do ƒë√≥\",\"h·ªÖ\",\"l·∫´n\",\"c√πng\",\n",
    "              \"ngo√†i ra\",\"v·∫≠y\",\"tr·ª´ phi\",\"h∆°n\",\"b·∫±ng\",\"nh∆∞\",\"d√π cho\",\n",
    "              \"nh·∫±m\",\"v√¨ v·∫≠y\",\"ng∆∞·ª£c l·∫°i\",\"ch·ªâ\",\"ch·ªâ tr·ª´\"}\n",
    "    lit_terms = load_lit_terms(\"lit_terms.txt\")\n",
    "    svd = TruncatedSVD(n_components=128) \n",
    "    input_file = r\"Embedding\\Output_ws\\ori_questions.json\"\n",
    "    output_file = r\"Embedding\\Output_features\\question_features.csv\"\n",
    "    print(\"------------------------ƒêang t√≠nh to√°n ƒë·ªô ph·ª©c t·∫°p gi·ªØa c√°c options------------------------\")\n",
    "    run_sen_comlex(input_file,output_file, connectors, phobert_tokenizer\n",
    "                   , phobert_model, gpt2_tokenizer, gpt2_model, device, lit_terms, svd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec6b21b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merger_data():\n",
    "    print(f\"--------------------------------------ƒêang k·∫øt h·ª£p data--------------------------------------\")\n",
    "    df1 = pd.read_csv(r\"Embedding\\Output_features\\noise_features.csv\")\n",
    "    df2 = pd.read_csv(r\"Embedding\\Output_features\\question_features.csv\")\n",
    "    df3 = pd.read_csv(r\"training\\data_for_training\\van_with_bloom_out.csv\")\n",
    "    merged = pd.merge(df1, df2, on = \"id\", how = \"inner\")\n",
    "    merged = pd.merge(merged, df3, on = \"id\", how = \"inner\")\n",
    "    result = merged.drop(columns=[\"question_x\", \"answer\", \"subject\", \"question_y\",\"question\"])\n",
    "    result.to_csv(\"training\\merged.csv\", index = False)\n",
    "    print(\"Done.........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1ef2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lit_terms():\n",
    "    df1 = pd.read_csv(r\"create_lit_terms\\bold_words.csv\", encoding=\"utf-8-sig\")\n",
    "    df2 = pd.read_csv(r\"create_lit_terms\\bold_words_2.csv\", encoding = \"utf-8-sig\")\n",
    "\n",
    "    df1[\"Bold Words\"] = df1[\"Bold Words\"].str.lower()\n",
    "    df2[\"Bold Words\"] = df2[\"Bold Words\"].str.lower()\n",
    "\n",
    "    merged = pd.concat([df1, df2], ignore_index=True)\n",
    "    unique_words = merged.drop_duplicates(subset=[\"Bold Words\"], keep=\"first\")[\"Bold Words\"]\n",
    "    # Xu·∫•t TXT\n",
    "    output_file = \"lit_terms.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for word in unique_words:\n",
    "            f.write(word.strip() + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ t·∫°o file {output_file}\")\n",
    "    print(\"üî¢ S·ªë l∆∞·ª£ng t·ª´ duy nh·∫•t:\", len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fc582fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------ƒêang x·ª≠ l√Ω Word Segment--------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Word Segmenting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40013/40013 [07:21<00:00, 90.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Word segmentation done.\n",
      "---------ƒê√£ x·ª≠ l√Ω xong Word Segment----------\n",
      "\n",
      "---------ƒêang x·ª≠ l√Ω ws_question--------------\n",
      "\n",
      "‚úÖ ƒê√£ chuy·ªÉn 3097 c√¢u h·ªèi sang JSON\n",
      "---------ƒê√£ x·ª≠ l√Ω xong ws_question----------\n",
      "\n",
      "---------ƒêang x·ª≠ l√Ω ori_question--------------\n",
      "\n",
      "‚úÖ ƒê√£ chuy·ªÉn th√†nh c√¥ng 3097 c√¢u h·ªèi sang file json\n",
      "---------ƒê√£ x·ª≠ l√Ω xong ori_question----------\n",
      "\n",
      "‚úÖ ƒê√£ t·∫°o file lit_terms.txt\n",
      "üî¢ S·ªë l∆∞·ª£ng t·ª´ duy nh·∫•t: 213\n",
      "---------ƒêang t√≠nh to√°n ƒë·ªô l·ªách embedding gi·ªØa c√°c options-------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 56/3097 [00:12<20:15,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L·ªói khi embedding c√¢u h·ªèi id=59, m√¥n=VƒÉn_h·ªçc\n",
      "L·ªói khi embedding c√¢u h·ªèi id=60, m√¥n=VƒÉn_h·ªçc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñä         | 265/3097 [00:53<09:15,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L·ªói khi embedding c√¢u h·ªèi id=280, m√¥n=VƒÉn_h·ªçc\n",
      "L·ªói khi embedding c√¢u h·ªèi id=282, m√¥n=VƒÉn_h·ªçc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñâ         | 273/3097 [00:55<06:54,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L·ªói khi embedding c√¢u h·ªèi id=288, m√¥n=VƒÉn_h·ªçc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñâ         | 275/3097 [00:55<06:01,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L·ªói khi embedding c√¢u h·ªèi id=290, m√¥n=VƒÉn_h·ªçc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1729/3097 [05:33<14:07,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L·ªói khi embedding c√¢u h·ªèi id=1813, m√¥n=VƒÉn_h·ªçc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1803/3097 [05:53<11:46,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L·ªói khi embedding c√¢u h·ªèi id=1891, m√¥n=VƒÉn_h·ªçc\n",
      "L·ªói khi embedding c√¢u h·ªèi id=1892, m√¥n=VƒÉn_h·ªçc\n",
      "L·ªói khi embedding c√¢u h·ªèi id=1893, m√¥n=VƒÉn_h·ªçc\n",
      "L·ªói khi embedding c√¢u h·ªèi id=1894, m√¥n=VƒÉn_h·ªçc\n",
      "L·ªói khi embedding c√¢u h·ªèi id=1895, m√¥n=VƒÉn_h·ªçc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1886/3097 [06:14<04:17,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L·ªói khi embedding c√¢u h·ªèi id=1978, m√¥n=VƒÉn_h·ªçc\n",
      "L·ªói khi embedding c√¢u h·ªèi id=1979, m√¥n=VƒÉn_h·ªçc\n",
      "L·ªói khi embedding c√¢u h·ªèi id=1980, m√¥n=VƒÉn_h·ªçc\n",
      "L·ªói khi embedding c√¢u h·ªèi id=1981, m√¥n=VƒÉn_h·ªçc\n",
      "L·ªói khi embedding c√¢u h·ªèi id=1982, m√¥n=VƒÉn_h·ªçc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2039/3097 [06:46<04:46,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L·ªói khi embedding c√¢u h·ªèi id=2133, m√¥n=VƒÉn_h·ªçc\n",
      "L·ªói khi embedding c√¢u h·ªèi id=2134, m√¥n=VƒÉn_h·ªçc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3097/3097 [10:24<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------ƒê√£ ho√†n th√†nh xong feartures t√≠nh ƒë·ªô l·ªách nhau gi·ªØa c√°c options-------------\n",
      " Embedding\\Output_features\\noise_features.csv\n",
      "‚ö° Using device: cpu\n",
      "üìñ ƒê√£ load 212 thu·∫≠t ng·ªØ t·ª´ lit_terms.txt\n",
      "------------------------ƒêang t√≠nh to√°n ƒë·ªô l·ªách gi·ªØa c√°c options------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   0%|          | 0/3097 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Processing questions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3097/3097 [06:58<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ xu·∫•t ƒë·ªô ph·ª©c t·∫°p c·ªßa c√¢u h·ªèi sang file CSV: Embedding\\Output_features\\question_features.csv\n",
      "   id  subject                                           question  \\\n",
      "0   1  VƒÉn_h·ªçc  T·ª´  VƒÉn h√≥a trong vƒÉn b·∫£n  Phong c√°ch H·ªì Ch√≠ M...   \n",
      "1   2  VƒÉn_h·ªçc  √ù n√†o n√≥i l√™n vi·ªác ti·∫øp thu c√≥ ch·ªçn l·ªçc tinh h...   \n",
      "2   3  VƒÉn_h·ªçc  V·∫•n ƒë·ªÅ ch·ªß y·∫øu ƒë∆∞·ª£c n√≥i t·ªõi trong vƒÉn b·∫£n Phon...   \n",
      "3   4  VƒÉn_h·ªçc  Theo t√°c gi·∫£ quan ni·ªám th·∫©m mƒ© v·ªÅ cu·ªôc s·ªëng c·ªß...   \n",
      "4   5  VƒÉn_h·ªçc         T·ª´ n√†o sau ƒë√¢y tr√°i nghƒ©a v·ªõi tru√¢n chuy√™n   \n",
      "\n",
      "   sentence_length  avg_word_length  num_clauses  num_punct  num_nouns  \\\n",
      "0                8         6.250000            0          0          4   \n",
      "1               19         5.000000            0          0          7   \n",
      "2               11         5.454545            0          0          4   \n",
      "3               11         5.818182            0          0          5   \n",
      "4                8         4.375000            1          0          2   \n",
      "\n",
      "   num_verbs  num_adjs  ...   emb_118   emb_119   emb_120   emb_121   emb_122  \\\n",
      "0          1         0  ... -0.092035 -0.013289  0.086528  0.049509 -0.012590   \n",
      "1          8         0  ... -0.151438  0.269236  0.109473  0.191930  0.340149   \n",
      "2          3         1  ... -0.115207  0.119910 -0.056836  0.262216 -0.318201   \n",
      "3          3         0  ... -0.054284 -0.154481  0.057942  0.029114 -0.079631   \n",
      "4          1         1  ... -0.065860 -0.255115  0.102225 -0.112054 -0.041635   \n",
      "\n",
      "    emb_123   emb_124   emb_125   emb_126   emb_127  \n",
      "0  0.060523 -0.189634  0.264915  0.158600  0.047479  \n",
      "1 -0.091962  0.269836  0.101109 -0.145045  0.164242  \n",
      "2  0.047343  0.067554  0.193902  0.038842  0.034481  \n",
      "3 -0.069470  0.272596 -0.039150 -0.045145  0.069785  \n",
      "4 -0.031163 -0.092182  0.023228  0.063303  0.307079  \n",
      "\n",
      "[5 rows x 140 columns]\n",
      "--------------------------------------ƒêang k·∫øt h·ª£p data--------------------------------------\n",
      "Done.........\n"
     ]
    }
   ],
   "source": [
    "# data_process()\n",
    "run_embedding()\n",
    "\n",
    "create_lit_terms()\n",
    "create_different_feartures()\n",
    "create_sentence_comlex_feartures()\n",
    "merger_data()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
