{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55115092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vncorenlp nbimporter\n",
    "# !pip install pandas transformers torch underthesea vncorenlp tqdm hf_xet stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028fdfb8",
   "metadata": {},
   "source": [
    "data_process->word_segment ->Ã£Ã£Ã£Ã£(emb_different + complex) -> merged -> training-> check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43e92a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "import importlib\n",
    "\n",
    "from vncorenlp import VnCoreNLP\n",
    "import json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import stanza\n",
    "#import m√¥ h√¨nh\n",
    "from underthesea import word_tokenize, pos_tag\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "#import file\n",
    "from data_process import run_data_processed, clean_question, clean_word\n",
    "from Embedding.word_segment import word_segment, ws_question, ori_question\n",
    "from Embedding.emb_different import get_diff\n",
    "from Embedding.sen_complex import run_sen_comlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17919d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process():\n",
    "    subject_files = {\n",
    "    \"VƒÉn_h·ªçc\": \"output/vƒÉn.json\",\n",
    "    # \"s·ª≠\": \"output/s·ª≠.json\",\n",
    "    # \"ƒë·ªãa\": \"output/ƒë·ªãa.json\",\n",
    "    # \"anh\": \"output/anh_vƒÉn.json\"\n",
    "    }\n",
    "    all_data = {}\n",
    "    for subject, file_path in subject_files.items():\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                all_data[subject] = json.load(f)\n",
    "            print(f\"Loaded {len(all_data[subject])} questions from {subject}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found - {file_path}\")\n",
    "            continue\n",
    "    qa_pairs = []\n",
    "    processed_qa = []\n",
    "    clean_question(qa_pairs, all_data)\n",
    "    clean_word(processed_qa, qa_pairs)\n",
    "    run_data_processed(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4337f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_embedding():\n",
    "    input_file = r\"data_processed\\VƒÉn_h·ªçc_processed.txt\"\n",
    "    output_file = r\"Embedding\\Output_ws\\qa_processed_ws.txt\"\n",
    "    print(\"---------ƒêang x·ª≠ l√Ω Word Segment--------------\\n\")\n",
    "    word_segment(input_file, output_file)\n",
    "    print(\"---------ƒê√£ x·ª≠ l√Ω xong Word Segment----------\\n\")\n",
    "\n",
    "    #------------- x·ª≠ l√Ω data word segment--------------------------------\n",
    "    print(\"---------ƒêang x·ª≠ l√Ω ws_question--------------\\n\")\n",
    "    output_file_ws = r\"Embedding\\Output_ws\\questions.json\"\n",
    "    ws_question(output_file, output_file_ws)\n",
    "    print(\"---------ƒê√£ x·ª≠ l√Ω xong ws_question----------\\n\")\n",
    "\n",
    "    #-------------clean data nguy√™n b·∫£n-----------------------\n",
    "    print(\"---------ƒêang x·ª≠ l√Ω ori_question--------------\\n\")\n",
    "    output_file_ori = r\"Embedding\\Output_ws\\ori_questions.json\"\n",
    "    ori_question(input_file, output_file_ori)\n",
    "    print(\"---------ƒê√£ x·ª≠ l√Ω xong ori_question----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa3278e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_different_feartures():\n",
    "    input = r\"Embedding\\Output_ws\\questions.json\"\n",
    "    output = r\"Embedding\\Output_features\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "    model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "    print(f\"---------ƒêang t√≠nh to√°n ƒë·ªô l·ªách embedding gi·ªØa c√°c options-------------\\n\")\n",
    "    get_diff(input, output, tokenizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d972142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lit_terms(txt_file=r\"create_lit_terms\\lit_terms.txt\"):\n",
    "    \"\"\"\n",
    "    ƒê·ªçc danh s√°ch thu·∫≠t ng·ªØ t·ª´ file txt th√†nh set\n",
    "    \"\"\"\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        terms = {line.strip().lower() for line in f if line.strip()}\n",
    "    print(f\"üìñ ƒê√£ load {len(terms)} thu·∫≠t ng·ªØ t·ª´ {txt_file}\")\n",
    "    return terms\n",
    "\n",
    "def create_sentence_complex_features():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"‚ö° Using device:\", device)\n",
    "    # PhoBERT cho embedding\n",
    "    phobert_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "    phobert_model = AutoModel.from_pretrained(\"vinai/phobert-base\").to(device)\n",
    "    phobert_model.eval()\n",
    "    # GPT2 ti·∫øng Vi·ªát cho perplexity\n",
    "    gpt2_tokenizer = AutoTokenizer.from_pretrained(\"NlpHUST/gpt2-vietnamese\")\n",
    "    gpt2_model = AutoModelForCausalLM.from_pretrained(\"NlpHUST/gpt2-vietnamese\").to(device)\n",
    "    gpt2_model.eval()\n",
    "\n",
    "    stanza.download('vi') \n",
    "    nlp = stanza.Pipeline('vi') \n",
    "\n",
    "    connectors = {\"v√†\",\"ho·∫∑c\",\"nh∆∞ng\",\"tuy nhi√™n\",\"c≈©ng nh∆∞ l√†\",\"n√™n\",\"m√†\",\n",
    "              \"n·∫øu\",\"th√¨\",\"b·ªüi v√¨\",\"v√¨\",\"khi\",\"m·∫∑c d√π\",\"ƒë·ªÉ\",\"sau\",\"sau_khi\",\n",
    "              \"tr∆∞·ªõc\",\"tr∆∞·ªõc khi\",\"hay\",\"do\",\"do ƒë√≥\",\"h·ªÖ\",\"l·∫´n\",\"c√πng\",\n",
    "              \"ngo√†i ra\",\"v·∫≠y\",\"tr·ª´ phi\",\"h∆°n\",\"b·∫±ng\",\"nh∆∞\",\"d√π cho\",\n",
    "              \"nh·∫±m\",\"v√¨ v·∫≠y\",\"ng∆∞·ª£c l·∫°i\",\"ch·ªâ\",\"ch·ªâ tr·ª´\"}\n",
    "    lit_terms = load_lit_terms(\"lit_terms.txt\")\n",
    "    svd = TruncatedSVD(n_components=128) \n",
    "    input_file = r\"Embedding\\Output_ws\\ori_questions.json\"\n",
    "    output_file = r\"Embedding\\Output_features\\question_features.csv\"\n",
    "    print(\"------------------------ƒêang t√≠nh to√°n ƒë·ªô ph·ª©c t·∫°p gi·ªØa c√°c options------------------------\")\n",
    "    run_sen_comlex(input_file,output_file, connectors, phobert_tokenizer\n",
    "                   , phobert_model, gpt2_tokenizer, gpt2_model, device, lit_terms, svd, nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec6b21b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merger_data():\n",
    "    print(f\"--------------------------------------ƒêang k·∫øt h·ª£p data--------------------------------------\")\n",
    "    df1 = pd.read_csv(r\"Embedding\\Output_features\\noise_features.csv\")\n",
    "    df2 = pd.read_csv(r\"Embedding\\Output_features\\question_features.csv\")\n",
    "    df3 = pd.read_csv(r\"training\\data_for_training\\van_with_bloom_out.csv\")\n",
    "    merged = pd.merge(df1, df2, on = \"id\", how = \"inner\")\n",
    "    merged = pd.merge(merged, df3, on = \"id\", how = \"inner\")\n",
    "    result = merged.drop(columns=[\"question_x\", \"answer\", \"subject\", \"question_y\",\"question\"])\n",
    "    result.to_csv(\"training\\merged.csv\", index = False)\n",
    "    print(\"Done.........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1ef2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lit_terms():\n",
    "    df1 = pd.read_csv(r\"create_lit_terms\\bold_words.csv\", encoding=\"utf-8-sig\")\n",
    "    df2 = pd.read_csv(r\"create_lit_terms\\bold_words_2.csv\", encoding = \"utf-8-sig\")\n",
    "\n",
    "    df1[\"Bold Words\"] = df1[\"Bold Words\"].str.lower()\n",
    "    df2[\"Bold Words\"] = df2[\"Bold Words\"].str.lower()\n",
    "\n",
    "    merged = pd.concat([df1, df2], ignore_index=True)\n",
    "    unique_words = merged.drop_duplicates(subset=[\"Bold Words\"], keep=\"first\")[\"Bold Words\"]\n",
    "    # Xu·∫•t TXT\n",
    "    output_file = \"lit_terms.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for word in unique_words:\n",
    "            f.write(word.strip() + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ t·∫°o file {output_file}\")\n",
    "    print(\"üî¢ S·ªë l∆∞·ª£ng t·ª´ duy nh·∫•t:\", len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc582fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ t·∫°o file lit_terms.txt\n",
      "üî¢ S·ªë l∆∞·ª£ng t·ª´ duy nh·∫•t: 213\n",
      "‚ö° Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:03, 127kB/s]                     \n",
      "2025-10-09 07:20:02 INFO: Downloaded file to C:\\Users\\quang\\stanza_resources\\resources.json\n",
      "2025-10-09 07:20:02 INFO: Downloading default packages for language: vi (Vietnamese) ...\n",
      "2025-10-09 07:20:04 INFO: File exists: C:\\Users\\quang\\stanza_resources\\vi\\default.zip\n",
      "2025-10-09 07:20:09 INFO: Finished downloading models and saved to C:\\Users\\quang\\stanza_resources\n",
      "2025-10-09 07:20:09 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 1.84MB/s]                    \n",
      "2025-10-09 07:20:09 INFO: Downloaded file to C:\\Users\\quang\\stanza_resources\\resources.json\n",
      "2025-10-09 07:20:11 INFO: Loading these models for language: vi (Vietnamese):\n",
      "================================\n",
      "| Processor    | Package       |\n",
      "--------------------------------\n",
      "| tokenize     | vtb           |\n",
      "| pos          | vtb_charlm    |\n",
      "| lemma        | identity      |\n",
      "| constituency | vlsp22_charlm |\n",
      "| depparse     | vtb_charlm    |\n",
      "| sentiment    | vsfc_charlm   |\n",
      "| ner          | vlsp          |\n",
      "================================\n",
      "\n",
      "2025-10-09 07:20:11 INFO: Using device: cpu\n",
      "2025-10-09 07:20:11 INFO: Loading: tokenize\n",
      "c:\\Users\\quang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "2025-10-09 07:20:14 INFO: Loading: pos\n",
      "2025-10-09 07:20:16 INFO: Loading: lemma\n",
      "2025-10-09 07:20:16 INFO: Loading: constituency\n",
      "2025-10-09 07:20:16 INFO: Loading: depparse\n",
      "2025-10-09 07:20:16 INFO: Loading: sentiment\n",
      "2025-10-09 07:20:17 INFO: Loading: ner\n",
      "2025-10-09 07:20:19 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ ƒê√£ load 212 thu·∫≠t ng·ªØ t·ª´ lit_terms.txt\n",
      "------------------------ƒêang t√≠nh to√°n ƒë·ªô ph·ª©c t·∫°p gi·ªØa c√°c options------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3097/3097 [54:23<00:00,  1.05s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ xu·∫•t ƒë·ªô ph·ª©c t·∫°p c·ªßa c√¢u h·ªèi sang file CSV: Embedding\\Output_features\\question_features.csv\n",
      "   id  subject                                           question  \\\n",
      "0   1  VƒÉn_h·ªçc  T·ª´  VƒÉn h√≥a trong vƒÉn b·∫£n  Phong c√°ch H·ªì Ch√≠ M...   \n",
      "1   2  VƒÉn_h·ªçc  √ù n√†o n√≥i l√™n vi·ªác ti·∫øp thu c√≥ ch·ªçn l·ªçc tinh h...   \n",
      "2   3  VƒÉn_h·ªçc  V·∫•n ƒë·ªÅ ch·ªß y·∫øu ƒë∆∞·ª£c n√≥i t·ªõi trong vƒÉn b·∫£n Phon...   \n",
      "3   4  VƒÉn_h·ªçc  Theo t√°c gi·∫£ quan ni·ªám th·∫©m mƒ© v·ªÅ cu·ªôc s·ªëng c·ªß...   \n",
      "4   5  VƒÉn_h·ªçc         T·ª´ n√†o sau ƒë√¢y tr√°i nghƒ©a v·ªõi tru√¢n chuy√™n   \n",
      "\n",
      "   sentence_length  avg_word_length  num_clauses  num_punct  num_nouns  \\\n",
      "0                8         6.250000            0          0          4   \n",
      "1               19         5.000000            0          0          7   \n",
      "2               11         5.454545            0          0          4   \n",
      "3               11         5.818182            0          0          5   \n",
      "4                8         4.375000            1          0          2   \n",
      "\n",
      "   num_verbs  num_adjs  ...   emb_118   emb_119   emb_120   emb_121   emb_122  \\\n",
      "0          1         0  ... -0.121395  0.038703  0.105827  0.140186 -0.088233   \n",
      "1          8         0  ...  0.088850 -0.099665  0.208452  0.051710 -0.288680   \n",
      "2          3         1  ... -0.026189  0.067642  0.330331 -0.249019  0.138513   \n",
      "3          3         0  ...  0.000286  0.123013 -0.033084  0.014948  0.185388   \n",
      "4          1         1  ... -0.046252  0.226547 -0.149179  0.138638  0.010852   \n",
      "\n",
      "    emb_123   emb_124   emb_125   emb_126   emb_127  \n",
      "0  0.040415  0.114341 -0.072899 -0.085841 -0.036751  \n",
      "1 -0.209916 -0.216252 -0.096221  0.102803  0.105517  \n",
      "2 -0.016551  0.057328 -0.120496 -0.186890 -0.063579  \n",
      "3  0.024102 -0.089544 -0.077923  0.106200 -0.151287  \n",
      "4 -0.165091 -0.027690 -0.078611  0.022675 -0.188811  \n",
      "\n",
      "[5 rows x 141 columns]\n",
      "--------------------------------------ƒêang k·∫øt h·ª£p data--------------------------------------\n",
      "Done.........\n"
     ]
    }
   ],
   "source": [
    "data_process()\n",
    "run_embedding()\n",
    "create_lit_terms()\n",
    "create_sentence_complex_features()\n",
    "merger_data()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
