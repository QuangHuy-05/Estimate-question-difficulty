{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbfc687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vncorenlp\n",
      "  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.6 MB 325.1 kB/s eta 0:00:09\n",
      "      --------------------------------------- 0.1/2.6 MB 544.7 kB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.1/2.6 MB 798.9 kB/s eta 0:00:04\n",
      "     --- ------------------------------------ 0.3/2.6 MB 1.3 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 0.5/2.6 MB 2.0 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 1.0/2.6 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 2.0/2.6 MB 6.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 7.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting requests (from vncorenlp)\n",
      "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/1e/db/4254e3eabe8020b458f1a747140d32277ec7a271daf1d235b70dc0b4e6e3/requests-2.32.5-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->vncorenlp)\n",
      "  Obtaining dependency information for charset_normalizer<4,>=2 from https://files.pythonhosted.org/packages/39/f5/3b3836ca6064d0992c58c7561c6b6eee1b3892e9665d650c803bd5614522/charset_normalizer-3.4.3-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading charset_normalizer-3.4.3-cp312-cp312-win_amd64.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->vncorenlp)\n",
      "  Obtaining dependency information for idna<4,>=2.5 from https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl.metadata\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->vncorenlp)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/a7/c2/fe1e52489ae3122415c51f387e221dd0773709bad6c6cdaa599e8a2c5185/urllib3-2.5.0-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->vncorenlp)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/e5/48/1549795ba7742c948d2ad169c1c8cdbae65bc450d6cd753d124b17c8cd32/certifi-2025.8.3-py3-none-any.whl.metadata\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.7/64.7 kB ? eta 0:00:00\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp312-cp312-win_amd64.whl (107 kB)\n",
      "   ---------------------------------------- 0.0/107.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 107.5/107.5 kB ? eta 0:00:00\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "   ---------------------------------------- 0.0/129.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 129.8/129.8 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: vncorenlp\n",
      "  Building wheel for vncorenlp (pyproject.toml): started\n",
      "  Building wheel for vncorenlp (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2646070 sha256=afcf405b27c3c522810b4594053223399c5cfa8949d85f531f6d8d495526537e\n",
      "  Stored in directory: c:\\users\\quang\\appdata\\local\\pip\\cache\\wheels\\6f\\19\\20\\ec7083125fd06db1a19d0d3ca18806ecf4e8ed1464713b4efa\n",
      "Successfully built vncorenlp\n",
      "Installing collected packages: urllib3, idna, charset_normalizer, certifi, requests, vncorenlp\n",
      "Successfully installed certifi-2025.8.3 charset_normalizer-3.4.3 idna-3.10 requests-2.32.5 urllib3-2.5.0 vncorenlp-1.0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install vncorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327dd942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hồ_Chí_Minh', 'là', 'lãnh_tụ', 'vĩ_đại']]\n"
     ]
    }
   ],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "\n",
    "rdrsegmenter = VnCoreNLP(\n",
    "    r\"D:\\Download\\VnCoreNLP-1.1.1\\VnCoreNLP-1.1.1\\VnCoreNLP-1.1.1.jar\",\n",
    "    annotators=\"wseg\",\n",
    "    max_heap_size='-Xmx2g'\n",
    ")\n",
    "print(rdrsegmenter.tokenize(\"Hồ Chí Minh là lãnh tụ vĩ đại\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e3aa680",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = r\"D:\\NCKH\\Tokenizer\\questions_cleaned.txt\"\n",
    "output_file = r\"D:\\NCKH\\Tokenizer\\word_segment_ques.txt\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            segmented = rdrsegmenter.tokenize(line)\n",
    "            f.write(\" \".join(segmented[0]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8522cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
